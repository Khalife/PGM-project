%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}  
\usepackage[T1]{fontenc} 
\usepackage[top=1.5cm,bottom=1.5cm,left=1.3cm,right=1cm,asymmetric]{geometry}

\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{array,multirow,makecell}
%\usepackage{cancel}
\usepackage{subfig}
\usepackage{float}
%\usepackage{wrapfig}
\usepackage[]{algorithm2e}
\setcellgapes{1pt}
\makegapedcells
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash }b{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash }b{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash }b{#1}}



\usepackage{tikz}
\usetikzlibrary{arrows,automata,fit}

\pagestyle{fancy}
\renewcommand{\footrulewidth}{1pt}
\fancyhead[R]{\textit{Master MVA : Probabilistic graphical models}}
\fancyfoot[L]{\textit{}}
%\usepackage{unicode-math}
%\setmathfont{XITS Math}
%\setmathfont[version=setB,StylisticSet=1]{XITS Math}


%\geometry{hmargin=1.5cm,vmargin=2cm}   

\begin{document}
\begin{center}
\section*{Latent dirichlet allocation for label modelling}
\section*{Thibaud Ehret \& Sammy Khalife}
\end{center}
~\\
\section*{Introduction to document analysis}
Eventhough the Latent dirichlet method can be used to a wider class of applications, the first one is document labelling. To see the improvments and contributions of the LDA, it is interesting for us to consider the history of document analysis methods.
\begin{itemize}
\item \textbf{Term Frequency-Inverse Document Frequency (tf-idf),}~\\ \textit{Salton and McGill, 1983}~\\
This natural and basic method reduces the arbitrary length documents to a fixed length of numbers by counting their apparition in the document.~\\
\item \textbf{Latent semantic indexing}, \textit{Deerwester et al., 1990}~\\
This method is similar to principal component analysis. Uses decomposition of the tf-idf matrix. Preserving the most important semantic information~\\
\item \textbf{Probabilistic Latent Semantic Indexing}, \textit{Hoffmann 1999}~\\
Here, each word in a document is a sample from a mixture of multinomials. The document is represented as a list of mixing proportions. However, there is no providing of generative model, and the complexity is proportionnal to the size of the corpus.
\end{itemize}
\section{Presentation of the model [Journal of Machine Learning, 2003]}
\begin{figure}[!H]
	\centering
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.3cm,
		semithick]
		\tikzstyle{every state}=[fill=white,draw=black,text=black]
		\tikzstyle{final}=[circle, fill=black,draw=black,text=white]
		\node[state]         (A)                    {$\alpha$};
		\node[state]         (B) [right of=A]       {$\theta$};
		\node[state]         (C) [right of=B]       {$z$};
		\node[state]         (D) [above of=C]       {$\beta$};
		\node[final]         (E) [right of=C]       {$\textbf{w}$};
		\node (B1) [draw=red, fit= (E) (B) (C) , inner sep=2cm] {};
		\node (B2) [draw=red, fit= (C) (E), inner sep=1cm] {};
		\node [yshift=3.0ex, black] at (B1.south) {$M$};
		\node [yshift=3.0ex, black] at (B2.south) {$N$};
		\path (A) edge              (B)
	              (C) edge              (E)
	              (D) edge              (E)
	              (B) edge              (C);
	\end{tikzpicture}
	\caption{LDA model}
	\label{LDA_model}
\end{figure}
~\\
In the latent dirichlet model (LDA), which a graphical representation is given in figure \ref{LDA_model}, there are different levels of representation of the object concerned (documents in our case). It differs from a simple multinomial model since each document can be associated with multiple topics. ~\\
~\\
\textbf{Generation of documents}~\\
~\\
The parameter $\theta$  which belongs to $\mathbb{R}^{d}$ is sampled once per document from a smooth distribution on the topic simplex, and represents the distribution of the topics within the document. $\theta$ is generated through a dirichlet distribution of parameter $\alpha$. We suppose that each word $\textbf{w}$ is generated through a multinomial probability conditionned on the topic : $\beta \in \mathbb{R}^{K*V}$, with $\beta_{ij}=p(w^{j}=1|z^{i}=1)$, V representing the number of possible words, and K the number of topics.~\\
~\\
The fundamental theorical result which justifies this representation is De Finetii's theorem that we will admit. We here suppose that the sequence of words is infinitely exchangeable (i.e that every finite subsequence is exchangeable) which allows us to represent the distribution $p(w,z)$ with a random parameter $\theta$ of a multinomial over topics :
\begin{eqnarray*}
p(\textbf{w},z)=\int p(\theta)(\prod_{n=1}^{N}p(z_{n} | \theta)p(w_{n}|z_{n}))d\theta
\end{eqnarray*}
~\\
\textbf{Example}~\\
~\\
To understand the model, let us consider the example of 3 words and 4 topics.
Then, the (V-1) simplex $\{\theta \in \mathbb{R}^{3}, \sum_{i}\theta_{i}=1\}$  is included in $\mathbb{R}^{2}$ and can be seen as a 2D-triangle, in which the parameter $\theta$ belongs. Through $\theta$, this simplex represents all possible distributions $p(w|\theta,\beta)$ over the three words. This is represented in the \textbf{figure 2}.~\\
~\\
The surface represented over the triangle is an example of distribution over the simplex :
\begin{eqnarray*}
p(\theta | \alpha)=\frac{\Gamma(\sum_{i=1}^{k}\alpha_{i})}{\prod_{i=1}^{k}\Gamma(\alpha_{i})}\theta_{1}^{\alpha_{1}-1}...\theta_{k}^{\alpha_{k}-1}
\end{eqnarray*}
We can interpret each of the vertices of the triangle corresponds to a deterministic distribution that assigns probability one to one of the words, and the midpoint of an edge as a probability which gives 0.5 to two of the words, and the centroid of the triangle is the uniform distribution over all three words. The four points marked with an x are the locations of the multinomial distributions $p(w | z)$ for each of the four topics.
\begin{figure}[!h]
\centering
\includegraphics[width=11cm]{Simplex}
\caption{2D-simplex representation of the example}
\end{figure}
\begin{eqnarray*}
p(\textbf{w}|\alpha, \beta) &  = & \int p(\theta | \alpha, \beta)p(\textbf{w} | \alpha, \beta, \theta)d\theta\\
\end{eqnarray*}
\textbf{Details concerning the model and equations for inference}~\\
~\\
In the LDA model, we have $ \theta \perp \beta$, and $p(w_{i} | \alpha, \beta, \theta) \perp p(w_{j} | \alpha, \beta, \theta)$  for $ i \neq j $
\begin{eqnarray*}
p(\textbf{w}|\alpha, \beta) & = & \int p(\theta | \alpha) \prod_{n=1}^{N} p(\textbf{w}_{n} | \alpha, \beta, \theta)d\theta\\
& = & \int  p(\theta |\alpha) \prod_{n=1}^{N}  \sum_{z_{n}}p(z_{n}|\alpha, \beta, \theta)p(\textbf{w}_{n} | \alpha, \beta, \theta, z_{n}) d\theta\\
\end{eqnarray*}
Moreover $z \perp (\alpha,\beta)$ and $\textbf{w}_{n} \perp (\alpha,\theta)$, then
\begin{eqnarray*}
p(\textbf{w}|\alpha, \beta) & = & \int p(\theta | \alpha)  \prod_{n=1}^{N}  \sum_{z_{n}}p(z_{n} |\theta)p(\textbf{w}_{n} | z_{n}, \beta) d\theta\\
\end{eqnarray*}
Then the natural inferential problem that we need to solve in order to use LDA, is to compute :
$$p(\theta, \textbf{z}|\textbf{w},\alpha,\beta)=\frac{p(\theta,\textbf{z},\textbf{w}|\alpha,\beta)}{p(\textbf{w}|\alpha,\beta)}$$
Unfortunately, this expression is intractable due to the coupling between $\theta$ and $\beta$.
This is reason of the specific approximation algorithm presented in the next part. ~\\

\section{The algorithm in practice}
\input{algorithm.tex}
\section{Experiments}
\input{experiment.tex}
\section*{Conclusion}

In this project we studied the Latend Dirichlet Allocation which allows to model documents in a collection such as a corpus of text. Because of the complexity of the model, the parameter are hard to estimate therefore we had to study a second model used to do a variational inference so that we can compute a lower of the likelihood of the initial model. We then used the model on toy examples as well as on the 20Newsgroup dataset to be able to analyze the result of the algorithm.

One improvement which could have been interesting to study after this LDA model is the composite model made from the LDA model as well as the HMM model, which combine the topic modeling as long as the syntax and try to see the differences on the same dataset. 

\end{document}
